# generate_k5_long_from_k4_seeds.py
# Generates K5 LONG candidates by extending K4 seed strategies with 1 additional signal.
# Output is sharded CSV (part1..partN) with a timestamp in the filename.

import argparse
import ast
import csv
import glob
import json
import os
from datetime import datetime, timezone
from typing import Dict, List, Iterable, Tuple, Any


DEFAULT_SIGNALS = [
    "adx", "atr", "bollinger", "cci", "ema50", "ma200",
    "macd", "mfi", "obv", "roc", "rsi", "stoch",
]


def utc_ts_compact() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M-%S")


def parse_weights(s: str) -> List[float]:
    # Accept "0.1,0.2,..." or "0.1 0.2" or "0.1..1.0:0.1"
    s = s.strip()
    if ".." in s and ":" in s:
        # "0.1..1.0:0.1"
        left, step = s.split(":", 1)
        a_str, b_str = left.split("..", 1)
        a = float(a_str)
        b = float(b_str)
        st = float(step)
        out = []
        x = a
        # inclusive b with float tolerance
        while x <= b + 1e-12:
            out.append(round(float(x), 10))
            x += st
        return out

    parts = [p for p in s.replace(",", " ").split() if p]
    if not parts:
        raise ValueError("No weights provided.")
    return [float(p) for p in parts]


def read_seeds_csv(paths: List[str], column: str) -> Iterable[Dict[str, float]]:
    for path in paths:
        with open(path, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if column not in reader.fieldnames:
                raise ValueError(f"Seed CSV '{path}' missing required column '{column}'. Found: {reader.fieldnames}")
            for row in reader:
                raw = (row.get(column) or "").strip()
                if not raw:
                    continue
                # Seeds are stored as python dict strings, e.g. "{'rsi': 0.3, 'macd': 0.2, ...}"
                try:
                    d = ast.literal_eval(raw)
                except Exception:
                    # also accept JSON dict
                    d = json.loads(raw)
                if not isinstance(d, dict):
                    continue
                clean: Dict[str, float] = {}
                for k, v in d.items():
                    clean[str(k)] = float(v)
                yield clean


def stable_key(d: Dict[str, float]) -> Tuple[Tuple[str, float], ...]:
    return tuple(sorted(((k, float(v)) for k, v in d.items()), key=lambda x: x[0]))


def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)


def shard_writer(out_prefix: str, max_rows_per_part: int, fieldnames: List[str]) -> Any:
    """
    Yields (writer, close_fn, part_path) for each shard.
    """
    part = 1
    rows_in_part = 0
    f = None
    writer = None
    part_path = None

    def open_part():
        nonlocal part, rows_in_part, f, writer, part_path
        if f:
            f.close()
        part_path = f"{out_prefix}.part{part}.csv"
        f = open(part_path, "w", newline="", encoding="utf-8")
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        rows_in_part = 0

    def close():
        nonlocal f
        if f:
            f.close()
            f = None

    open_part()

    def write_row(row: Dict[str, Any]):
        nonlocal part, rows_in_part
        if rows_in_part >= max_rows_per_part:
            part += 1
            open_part()
        writer.writerow(row)
        rows_in_part += 1
        return part_path

    return write_row, close


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--seeds", required=True,
                    help="Seed CSV or glob pattern(s) for K4 seeds. Example: results/k4_long_regime/top_seeds*.csv")
    ap.add_argument("--seed-column", default="Combination",
                    help="Column name containing dict-string combinations in the seed file. Default: Combination")
    ap.add_argument("--signals", default=",".join(DEFAULT_SIGNALS),
                    help="Comma-separated list of all available signals. Default: 12-signal set.")
    ap.add_argument("--weights", default="0.1..1.0:0.1",
                    help="Weights for the added 5th signal. Examples: '0.1..1.0:0.1' or '0.1,0.2,0.3'")
    ap.add_argument("--out-dir", required=True,
                    help="Output directory for generated shards, e.g. data/k5_long_regime/")
    ap.add_argument("--out-prefix", default=None,
                    help="Optional explicit output file prefix (without .partN.csv). If omitted, autogenerated.")
    ap.add_argument("--max-rows-per-part", type=int, default=500000,
                    help="Max rows per shard part file. Default: 500000")
    ap.add_argument("--dedupe", type=int, default=1,
                    help="1 to dedupe by exact dict content, 0 to allow duplicates. Default: 1")
    ap.add_argument("--limit-seeds", type=int, default=0,
                    help="If >0, only use first N seeds (for smoke tests). Default: 0 (all)")
    args = ap.parse_args()

    # Expand seed globs (allow multiple patterns separated by ';')
    patterns = [p.strip() for p in args.seeds.split(";") if p.strip()]
    seed_paths: List[str] = []
    for pat in patterns:
        matches = sorted(glob.glob(pat))
        if matches:
            seed_paths.extend(matches)
        elif os.path.isfile(pat):
            seed_paths.append(pat)

    if not seed_paths:
        raise SystemExit(f"No seed files found for --seeds: {args.seeds}")

    signals = [s.strip() for s in args.signals.split(",") if s.strip()]
    weights = parse_weights(args.weights)

    ensure_dir(args.out_dir)
    ts = utc_ts_compact()
    out_prefix = args.out_prefix
    if not out_prefix:
        out_prefix = os.path.join(args.out_dir, f"strategies_k5_long_from_k4_seeds_{ts}")

    # Output schema: keep it simple and consistent: a single column "Combination"
    fieldnames = ["Combination"]
    write_row, close_parts = shard_writer(out_prefix, args.max_rows_per_part, fieldnames)

    seen = set()
    total_written = 0
    total_seeds = 0

    print(f"[{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}] Loading seeds from {len(seed_paths)} file(s)")
    for seed in read_seeds_csv(seed_paths, args.seed_column):
        total_seeds += 1
        if args.limit_seeds > 0 and total_seeds > args.limit_seeds:
            break

        seed_keys = set(seed.keys())
        missing = [s for s in signals if s not in seed_keys]
        if not missing:
            # If seed already uses all signals, we cannot extend to K5. Skip.
            continue

        # For K5, we add exactly ONE new signal (the 5th) from missing signals.
        for add_sig in missing:
            for w in weights:
                combo = dict(seed)
                combo[add_sig] = float(w)

                if args.dedupe:
                    k = stable_key(combo)
                    if k in seen:
                        continue
                    seen.add(k)

                combo_str = json.dumps({k: float(v) for k, v in sorted(combo.items())}, separators=(",", ":"))
                write_row({"Combination": combo_str})
                total_written += 1

                if total_written % 200000 == 0:
                    print(f"[{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}] Generated {total_written} rows")

    close_parts()

    print(f"[{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}] Done")
    print(f"Seeds processed: {min(total_seeds, args.limit_seeds) if args.limit_seeds > 0 else total_seeds}")
    print(f"Rows written: {total_written}")
    print(f"Output prefix: {out_prefix} (files: {out_prefix}.part1.csv ..)")


if __name__ == "__main__":
    main()
